---
title: 'Project 3: Tutorial'
author: "Carl Myers"
collaborator: "Edward Sung"
output: rmarkdown::html_vignette
vienette: >
  %\VignetteIndexEntry{Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message = FALSE}
library(PROJECT3)
library(tidyverse)
library(stat302package)
library(magrittr)
library(randomForest)
library(dplyr)
library(ggpubr)
```
```

The PROJECT3 package is a final project for STAT 302. It can be used for a variety of uses including linear modeling, k-nearest-neighbors, t-tests, and Random Forest Cross Validation. To install the package from github, run the following code.z

## Install Package from Github
```{r, eval=FALSE}
devtools::install_github("carldmyers/PROJECT3")
library(PROJECT3)
help(package = "PROJECT3", help_type = "html")
utils::browseVignettes(package = "PROJECT3")
```

# Tutorial for my_t_test()

All the tests demonstrated below use `lifeExp` data from `my_gapminder`.
# case I: alternative = "two.sided"
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &\neq 60.
  \end{align}
```{r}
data("my_gapminder")
my_t_test(my_gapminder$lifeExp, alternative = "two.sided", mu = 60)
```
From the test result, we notice that the p_value is greater than 0.05. Thus, it is not statistically significant have no enough evidence to reject the null hypothesis.

# case II: alternative = "less"
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &< 60.
  \end{align}
```{r}
my_t_test(my_gapminder$lifeExp, alternative = "less", mu = 60)
```
From the test result, we notice that the p_value is less than 0.05. Thus, it is not statistically significant have no enough evidence to reject the null hypothesis.

# case III: alternative = "greater"
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &> 60.
  \end{align}
```{r}
my_t_test(my_gapminder$lifeExp, alternative = "greater", mu = 60)
```
From the test result, we notice that the p_value is greater than 0.05. Thus, it is not statistically significant have no enough evidence to reject the null hypothesis.


# Tutorial for my_lm()
```{r, eval=FALSE}
test <- my_lm(formula = lifeExp ~ gdpPercap + continent, data = my_gapminder)
my_coef <- test[, 1]
my_matrix <- model.matrix(lifeExp ~ gdpPercap + continent, data = my_gapminder)
y_hat <- my_matrix %*% as.matrix(my_coef)
my_df <- data.frame("actual" = my_gapminder$lifeExp, "fitted" = y_hat, "continent" = my_gapminder$continent)
 ggplot(my_df, aes(x = actual, y = fitted, color = continent)) +
   geom_point() +
   geom_abline(slope = 1, intercept = 0, col = "black") + 
   theme_bw(base_size = 15) +
   labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") + 
   theme(plot.title = element_text(hjust = 0.5))
```
As shown, we notice that the difference of `lifeExp` between two observations is an unit in `gdpPercap`. Compared to the coeffiecients of different continents, `gdpPercap` has less influence on `lifeExp` than `continent`.

# Tutorial for my_knn_cv()
```{r}
knn_vector <- c(1 : 10)
tu_knn <- matrix(NA, nrow = 10, ncol = 3)
for (i in 1 : 10) {
  tu_knn[i, 1] <- knn_vector[i]
  # get the cv misclassification rate
  tu_knn[i, 3] <- my_knn_cv(train = my_gapminder[, 3 : 4], 
          cl = my_gapminder$continent, k_nn = knn_vector[i], k_cv = 5)$cv_err
  prediction <- my_knn_cv(train = my_gapminder[, 3 : 4], 
          cl = my_gapminder$continent, k_nn = knn_vector[i], k_cv = 5)$class
  # get tge training misclassification rate
  tu_knn[i, 2] <- sum(prediction != my_gapminder$continent) / length(my_gapminder$continent)
}
tu_knn <- data.frame("k_nn" = tu_knn[, 1], "training_misclassification_rate" = tu_knn[, 2], 
                     "CV_misclassification_rate" = tu_knn[, 3])
```


# Tutorial for my_rf_cv()
```{r, eval = F}
library(reshape2)
set.seed(400)
#since c(2, 5, 10)
cv_mse_2 <- rep(NA, 30)
cv_mse_5 <- rep(NA, 30)
cv_mse_10 <- rep(NA, 30)
for(i in 1 : 30) {
  cv_mse_2[i] <- my_rf_cv(2)
  cv_mse_5[i] <- my_rf_cv(5)
  cv_mse_10[i] <- my_rf_cv(10)
}
cv_mse <- as.data.frame(cbind(cv_mse_2, cv_mse_5, cv_mse_10))
# The melt function takes data in wide format and 
# stacks a set of columns into a single column of data. 
cv_mse_m <- melt(cv_mse)
ggplot(cv_mse_m, aes(factor(variable), value)) +
  geom_boxplot(fill = "green") +
  labs(x = "Folds", y = "CV Error", title = "Folds V.S. CV Error") +
  scale_x_discrete(labels = c("Two", "Five", "Ten")) +
  theme_bw(base_size = 15) +
  theme(plot.title = element_text(hjust = 0.5))
avg <- rep(NA, 3)
std_dev <- rep(NA, 3)
for(i in 1:length(cv_mse)) {
  avg[i] <- mean(cv_mse[, i])
  std_dev[i] <- sd(cv_mse[, i])
}
table1 <- as.data.frame(cbind(avg, std_dev))
table1
```
Once the numbers of folds increase, the cv errors increase. I think if we increase the number of folds, we can get more accurate values.
