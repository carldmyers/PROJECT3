---
title: "Project 3: PROJECT3 Tutorial"
author: "Carl Myers"
collaborator: "Edward Sung"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Project 3: PROJECT3 Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message = FALSE}
library(PROJECT3)
library(ggplot2)
library(tidyverse)
library(magrittr)
library(kableExtra)
library(class)
library(dplyr)
library(stats)
library(randomForest)
```

The PROJECT3 package is a final project for STAT 302. It can be used for a variety of uses including linear modeling, k-nearest-neighbors, t-tests, and Random Forest Cross Validation. To install the package from github, run the following code.z

## Install Package from Github
```{r, eval=FALSE}
devtools::install_github("carldmyers/PROJECT3")
library(PROJECT3)
help(package = "PROJECT3", help_type = "html")
utils::browseVignettes(package = "PROJECT3")
```

# Tutorial for my_t_test()

All the tests demonstrated below use `lifeExp` data from `my_gapminder`.
# case I: alternative = "two.sided"
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &\neq 60.
  \end{align}
```{r}
data("my_gapminder")
my_t_test(my_gapminder$lifeExp, alternative = "two.sided", mu = 60)
```
From the test result, we notice that the p_value is greater than 0.05. Thus, it is not statistically significant have no enough evidence to reject the null hypothesis.

# case II: alternative = "less"
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &< 60.
  \end{align}
```{r}
my_t_test(my_gapminder$lifeExp, alternative = "less", mu = 60)
```
From the test result, we notice that the p_value is less than 0.05. Thus, it is not statistically significant have no enough evidence to reject the null hypothesis.

# case III: alternative = "greater"
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &> 60.
  \end{align}
```{r}
my_t_test(my_gapminder$lifeExp, alternative = "greater", mu = 60)
```
From the test result, we notice that the p_value is greater than 0.05. Thus, it is not statistically significant have no enough evidence to reject the null hypothesis.


# Tutorial for my_lm()
```{r, eval=FALSE}
test <- my_lm(formula = lifeExp ~ gdpPercap + continent, data = my_gapminder)
my_coef <- test[, 1]
my_matrix <- model.matrix(lifeExp ~ gdpPercap + continent, data = my_gapminder)
y_hat <- my_matrix %*% as.matrix(my_coef)
my_df <- data.frame("actual" = my_gapminder$lifeExp, "fitted" = y_hat, "continent" = my_gapminder$continent)
 ggplot(my_df, aes(x = actual, y = fitted, color = continent)) +
   geom_point() +
   geom_abline(slope = 1, intercept = 0, col = "black") + 
   theme_bw(base_size = 15) +
   labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") + 
   theme(plot.title = element_text(hjust = 0.5))
```
As shown, we notice that the difference of `lifeExp` between two observations is an unit in `gdpPercap`. Compared to the coeffiecients of different continents, `gdpPercap` has less influence on `lifeExp` than `continent`.

# Tutorial for my_knn_cv()
```{r}
knn_vector <- c(1 : 10)
tu_knn <- matrix(NA, nrow = 10, ncol = 3)
for (i in 1 : 10) {
  tu_knn[i, 1] <- knn_vector[i]
  # get the cv misclassification rate
  tu_knn[i, 3] <- my_knn_cv(train = my_gapminder[, 3 : 4], 
          cl = my_gapminder$continent, k_nn = knn_vector[i], k_cv = 5)$cv_err
  prediction <- my_knn_cv(train = my_gapminder[, 3 : 4], 
          cl = my_gapminder$continent, k_nn = knn_vector[i], k_cv = 5)$class
  # get tge training misclassification rate
  tu_knn[i, 2] <- sum(prediction != my_gapminder$continent) / length(my_gapminder$continent)
}
tu_knn <- data.frame("k_nn" = tu_knn[, 1], "training_misclassification_rate" = tu_knn[, 2], 
                     "CV_misclassification_rate" = tu_knn[, 3])
```


# Tutorial for my_rf_cv()
```{r, eval=FALSE}
library(class)
library(tidyverse)
library(randomForest)
my_rf_cv <- function(k) {
  fold <- sample(rep(1:k, length = length(my_gapminder$lifeExp)))
  # data <- data.frame()
  mse <- rep(NA, k)
  # loop thru the folds
  for (i in 1:k) {
    data_train <- my_gapminder[fold != i, ] # Xi
    data_test <-  my_gapminder[fold == i, ]  # Xi star
    # Train our models
    cl_train <- my_gapminder$lifeExp[fold != i] # Yi
    cl_test <- my_gapminder$lifeExp[fold == i]  # Yi star
    model <- randomForest(lifeExp ~ gdpPercap, data = data_train, ntree = 100)
    predictions <- predict(model, data_test[, -1])
    mse[i] <- mean((predictions - cl_test)^2)
  }
  output <- mean(mse)
}
# ########################
k_vector <- c(2, 5, 10)
tutor_rf <- matrix(NA, nrow = 30, ncol = 4)
for (i in 1 : length(k_vector)) {
  for (k in 1 : 30) {
    tutor_rf[k, i] <- my_rf_cv(k = k_vector[i])
  }
}
```
